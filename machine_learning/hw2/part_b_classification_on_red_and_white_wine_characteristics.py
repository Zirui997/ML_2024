# -*- coding: utf-8 -*-
"""Part B: Classification on red and white wine characteristics

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qb1d_eS16XbB0UHfkUWhfo-y1WF8Ii2P
"""

# Part B: Classification on Red and White Wine Characteristics

#Step 1: Import the Red and White Wine Datasets



import pandas as pd

# Import the datasets with the correct separator
red_wine_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'
white_wine_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv'

# Read red and white wine data
red_wine = pd.read_csv(red_wine_url, sep=';')
white_wine = pd.read_csv(white_wine_url, sep=';')

# Display the first few rows to confirm import
print(red_wine.head())
print(white_wine.head())

#Step 2: Add a winetype Column and Combine the Datasets

# Add a new column "winetype" (0 for white wine, 1 for red wine)
white_wine['winetype'] = 0
red_wine['winetype'] = 1

# Combine both datasets into a single dataframe
wine_data = pd.concat([white_wine, red_wine], ignore_index=True)

# Check the combined dataset
wine_data.head()

#Step 3: Visualize the Univariate Distribution of the Target Feature and Explanatory Variables

import seaborn as sns
import matplotlib.pyplot as plt

# Choose some continuous features to visualize
features = ['alcohol', 'pH', 'density', 'winetype']

# Plot univariate distributions for the target and explanatory variables
plt.figure(figsize=(10, 6))
for i, feature in enumerate(features):
    plt.subplot(2, 2, i+1)
    sns.histplot(wine_data[feature], kde=True)
    plt.title(f'Distribution of {feature}')
plt.tight_layout()
plt.show()

#Step 4: Split Data into Training and Test Sets and Build Classification Models

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score

# Define features and target
X = wine_data.drop(columns=['winetype'])
y = wine_data['winetype']

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
log_reg = LogisticRegression(max_iter=1000)
penalized_log_reg = LogisticRegression(penalty='l2', max_iter=1000)
knn = KNeighborsClassifier()

# Evaluate using cross-validation
models = {'Logistic Regression': log_reg, 'Penalized Logistic Regression': penalized_log_reg, 'KNN': knn}

for name, model in models.items():
    scores = cross_val_score(model, X_train, y_train, cv=5)
    print(f"{name}: Mean CV Score = {scores.mean():.4f}")

#Step 5: Running the Models With and Without StandardScaler

from sklearn.preprocessing import StandardScaler

# Standardize the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Evaluate models with StandardScaler
for name, model in models.items():
    scores_scaled = cross_val_score(model, X_train_scaled, y_train, cv=5)
    print(f"{name} with StandardScaler: Mean CV Score = {scores_scaled.mean():.4f}")

Step 6: Tuning Parameters Using GridSearchCV

from sklearn.model_selection import GridSearchCV

# Define the parameter grid for KNN
param_grid_knn = {'n_neighbors': [3, 5, 7, 9]}

# Use GridSearchCV for KNN
grid_knn = GridSearchCV(KNeighborsClassifier(), param_grid_knn, cv=5)
grid_knn.fit(X_train_scaled, y_train)

print(f"KNN Best Parameters: {grid_knn.best_params_}")

#Step 7: Compare Coefficients of Logistic Regression and Penalized Logistic Regression

# Fit the models
log_reg.fit(X_train_scaled, y_train)
penalized_log_reg.fit(X_train_scaled, y_train)

# Compare coefficients
print("Logistic Regression Coefficients:", log_reg.coef_)
print("Penalized Logistic Regression Coefficients:", penalized_log_reg.coef_)

# Get the best estimator for KNN after tuning
knn_best = grid_knn.best_estimator_

# Perform cross-validation to get the final scores after scaling
log_reg_score = cross_val_score(log_reg, X_train_scaled, y_train, cv=5).mean()
penalized_log_reg_score = cross_val_score(penalized_log_reg, X_train_scaled, y_train, cv=5).mean()
knn_best_score = cross_val_score(knn_best, X_train_scaled, y_train, cv=5).mean()

# Output the scores for comparison
print(f"Logistic Regression Best Mean CV Score: {log_reg_score:.4f}")
print(f"Penalized Logistic Regression Best Mean CV Score: {penalized_log_reg_score:.4f}")
print(f"KNN Best Mean CV Score: {knn_best_score:.4f}")

# Find the best performing model based on the scores
if max(log_reg_score, penalized_log_reg_score, knn_best_score) == log_reg_score:
    best_model = 'Logistic Regression'
elif max(log_reg_score, penalized_log_reg_score, knn_best_score) == penalized_log_reg_score:
    best_model = 'Penalized Logistic Regression'
else:
    best_model = 'KNN'

print(f"The best performing model is: {best_model}")

#Step 7: Compare Coefficients of Logistic Regression and Penalized Logistic Regression

# Evaluate the models on the test set
log_reg_test_score = log_reg.score(X_test_scaled, y_test)
penalized_log_reg_test_score = penalized_log_reg.score(X_test_scaled, y_test)
knn_test_score = knn_best.score(X_test_scaled, y_test)

# Output test scores for comparison
print(f"Logistic Regression Test Set Score: {log_reg_test_score:.4f}")
print(f"Penalized Logistic Regression Test Set Score: {penalized_log_reg_test_score:.4f}")
print(f"KNN Test Set Score: {knn_test_score:.4f}")

# Choose the best model based on test set performance
if max(log_reg_test_score, penalized_log_reg_test_score, knn_test_score) == log_reg_test_score:
    best_model = 'Logistic Regression'
elif max(log_reg_test_score, penalized_log_reg_test_score, knn_test_score) == penalized_log_reg_test_score:
    best_model = 'Penalized Logistic Regression'
else:
    best_model = 'KNN'

print(f"The best performing model on the test set is: {best_model}")